# Model Deploy Execution Attack Lab

This project is an automated adversarial attack experimentation platform targeting the security of **binary-level AI model deployments**. It supports multiple mainstream inference engines (e.g., MNN, NCNN, ONNXRuntime) and various attack algorithms.

## ğŸš€ é¡¹ç›®æ¶æ„ (Project Architecture)

```
.
â”œâ”€â”€ hook_config/         # GDB é’©å­çš„ JSON é…ç½®æ–‡ä»¶
â”œâ”€â”€ outputs/             # æ”»å‡»è¿‡ç¨‹ä¸­ç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬å’Œæ—¥å¿—
â”œâ”€â”€ pre_attack_scripts/  # æ”»å‡»å‰çš„å‡†å¤‡è„šæœ¬ (å¦‚ç­›é€‰ "false" å›¾ç‰‡)
â”œâ”€â”€ resources/           # å­˜æ”¾æ‰€æœ‰æ”»å‡»æ‰€éœ€çš„èµ„æº
â”‚   â”œâ”€â”€ execution_files/ # ç¼–è¯‘å¥½çš„æ¨¡å‹å¯æ‰§è¡Œç¨‹åº
â”‚   â”œâ”€â”€ false_image_list/# "false" å›¾ç‰‡çš„åˆ—è¡¨æ–‡ä»¶
â”‚   â”œâ”€â”€ images/          # ç”¨äºæµ‹è¯•å’Œæ”»å‡»çš„åŸå§‹å›¾ç‰‡
â”‚   â””â”€â”€ models/          # AI æ¨¡å‹æ–‡ä»¶ (.onnx, .mnn, etc.)
â”œâ”€â”€ results/             # æ‰¹é‡å›¾åƒåˆ†ç±»çš„ç»“æœ
â”œâ”€â”€ scripts/             # è¾…åŠ©è„šæœ¬ (å¦‚ç¯å¢ƒå®‰è£…)
â””â”€â”€ src/                 # æ”»å‡»ç®—æ³•çš„æ ¸å¿ƒ Python æºç 
```

## ğŸ“Š å®Œæ•´å·¥ä½œæµç¨‹ (End-to-End Workflow)

Follow these steps to set up the environment and run an attack.

### æ­¥éª¤ 1ï¼šç¯å¢ƒè®¾ç½® (Setup Environment)

1.  **ç¼–è¯‘ C++ ä»£ç **:
    Compile the C++ source code using the provided `CMakeLists.txt` file.
    ```bash
    mkdir build
    cd build
    cmake ..
    make
    ```

2.  **å®‰è£…ä¾èµ–**:
    Install system libraries and Python packages using the provided script (tested on Ubuntu 24.04).
    ```bash
    bash scripts/install_dependencies.sh
    ```
    Activate the Python virtual environment:
    ```bash
    source scripts/.venv/bin/activate
    ```

### æ­¥éª¤ 2ï¼šå‡†å¤‡èµ„æºæ–‡ä»¶ (Prepare Assets)

- Move compiled executables (e.g., `emotion_ferplus_mnn`) to `resources/execution_files/`.
- Place model files (`.onnx`, `.mnn`, etc.) in `resources/models/`.
- Store images for analysis in `resources/images/`.

### æ­¥éª¤ 3ï¼š(å¯é€‰ä½†æ¨è) ç­›é€‰æ”»å‡»å€™é€‰å›¾ç‰‡ (Filter Attack Candidates)

To improve attack efficiency, you can first find images that the model already classifies as "false". These images are often better starting points for an attack.

The `generate_false_image_list.sh` script automates this process.

1.  **æˆæƒ**:
    ```bash
    chmod +x pre_attack_scripts/generate_false_image_list.sh
    ```

2.  **è¿è¡Œè„šæœ¬**:
    Provide the path to the executable you want to test.
    ```bash
    # Example for the emotion_ferplus_mnn model
    ./pre_attack_scripts/generate_false_image_list.sh resources/execution_files/emotion_ferplus_mnn
    ```
    The script will generate a file like `resources/false_image_list/emotion_ferplus_mnn_false_list.txt`, containing paths to all images classified as "false".

### æ­¥éª¤ 4ï¼šæ‰§è¡Œæ”»å‡» (Run an Attack)

Here are typical command-line examples for running attacks.

#### NES Attack (Gray-box, State-matching)
```bash
python3 src/attackers/nes_attack.py \
    --executable resources/execution_files/mnist_mnn \
    --model resources/models/mnist.mnn \
    --hooks hook_config/mnist_mnn_hook_config.json \
    --golden-image resources/images/mnist_sample/7/7_0.png \
    --image outputs/nes_attack_1/best_attack_image_nes_host.png \
    --output-dir outputs/nes_attack_1 \
    --iterations 200 \
    --learning-rate 20.0 \
    --population-size 200
```

#### CMA-ES Attack (Gray-box, State-matching)
```bash
python3 src/attackers/cmaes_attack.py \
    --executable resources/execution_files/mnist_mnn \
    --model resources/models/mnist.mnn \
    --hooks hook_config/mnist_mnn_hook_config.json \
    --golden-image resources/images/mnist_sample/7/7_0.png \
    --image resources/images/mnist_sample/0/0_0.png \
    --output-dir outputs/cmaes_attack_1 \
    --iterations 100 \
    --population-size 100
```

> For other attack algorithms, please refer to the help message of each script via the `-h` flag.

## ğŸ”¬ æ”¯æŒçš„æ”»å‡»ç®—æ³• (Supported Attack Algorithms)

- **CMA-ES**: A gray-box (state-matching) algorithm ideal for low- to medium-dimensional inputs. It uses internal model states obtained via GDB hooks to guide its optimization process.
- **NES (Natural Evolution Strategies)**: A gray-box (state-matching) algorithm suitable for high-dimensional inputs. It has low memory consumption and estimates gradients using internal states from GDB hooks.
- **Boundary Attack, HopSkipJump, Sign-OPT**: Black-box (decision-based) algorithms that are effective when only the final decision (true/false) of the model is of interest.

## ğŸª GDB é’©å­é…ç½® (GDB Hook Configuration)

The `hook_config/` directory contains JSON files that tell the attack scripts where to set GDB breakpoints to extract intermediate features from the model executable. You must ensure the `hook_xx.json` file matches the version of the executable to ensure GDB can hit breakpoints correctly.

## â“ å¸¸è§é—®é¢˜ (FAQ)

- **Memory Overflow**: CMA-ES can be memory-intensive with high-resolution images. It is advisable to resize images to smaller dimensions (e.g., 64x64, 128x128) first.
- **Image Format**: Ensure images are in a common format (JPEG/PNG) and can be read by OpenCV.
- **GDB Permissions**: If GDB fails to attach, check the `/proc/sys/kernel/yama/ptrace_scope` setting.
- **Decision-based Attacks**: These attacks require an original image (classified as `false`) and a starting adversarial image (classified as `true`), both of which must have identical dimensions.





